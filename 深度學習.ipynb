{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "77a5794e-9d63-45e3-a594-e52cfa0efa85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” è©•ä¼°é©—è­‰é›†...\n",
      "\n",
      "ğŸ¯ é©—è­‰é›† AUC:\n",
      "Gender : 0.9485\n",
      "Hand   : 0.9993\n",
      "Play   : 0.9124\n",
      "Level  : 0.9779\n",
      "Final Score: 0.9595\n",
      "\n",
      "é–‹å§‹ Test set é æ¸¬...\n",
      "æª¢æŸ¥ ID æ•¸é‡ï¼šé æœŸ 1430ï¼Œç›®å‰ç”¢å‡º 1430\n",
      "âœ… æ‰€æœ‰ ID èˆ‡ sample_submission.csv ä¸€è‡´ï¼\n",
      "âœ… çµæœå·²è¼¸å‡ºè‡³ sample_submission_predtry.csv\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import entropy, kurtosis, skew\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ==== è³‡æ–™è§£å£“èˆ‡è®€æª”ï¼ˆè«‹ä¾ç…§å¯¦éš›è·¯å¾‘ä¿®æ”¹ï¼‰====\n",
    "train_zip = r\"C:\\Users\\User\\Downloads\\39_Training_Dataset.zip\"\n",
    "test_zip = r\"C:\\Users\\User\\Downloads\\39_Test_Dataset.zip\"\n",
    "extract_root = \"aicup_data\"\n",
    "\n",
    "train_dir = os.path.join(extract_root, \"train\")\n",
    "test_dir = os.path.join(extract_root, \"test\")\n",
    "os.makedirs(train_dir, exist_ok=True)\n",
    "os.makedirs(test_dir, exist_ok=True)\n",
    "\n",
    "with zipfile.ZipFile(train_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(train_dir)\n",
    "with zipfile.ZipFile(test_zip, 'r') as zip_ref:\n",
    "    zip_ref.extractall(test_dir)\n",
    "\n",
    "train_inner = os.path.join(train_dir, \"39_Training_Dataset\")\n",
    "test_inner = os.path.join(test_dir, \"39_Test_Dataset\")\n",
    "\n",
    "train_info = pd.read_csv(os.path.join(train_inner, \"train_info.csv\"))\n",
    "test_info = pd.read_csv(os.path.join(test_inner, \"test_info.csv\"))\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# FFT èˆ‡é »åŸŸç‰¹å¾µç›¸é—œå‡½å¼ï¼ˆæ•´åˆè‡³ä¸»ç¨‹å¼æµç¨‹ï¼‰\n",
    "# =============================================================================\n",
    "def FFT(xreal, ximag):\n",
    "    \"\"\"\n",
    "    æ‰‹å‹•å¯¦ä½œ Cooley-Tukey FFT\n",
    "    åƒæ•¸:\n",
    "      xreal, ximag: è¼¸å…¥ä¿¡è™Ÿçš„å¯¦éƒ¨èˆ‡è™›éƒ¨ï¼ˆlist æˆ– arrayï¼‰ï¼Œé•·åº¦è‡³å°‘ç‚º 2 çš„å†ªæ¬¡\n",
    "    å›å‚³:\n",
    "      n, xreal, ximagï¼šn ç‚ºå¯¦éš›æ¡ç”¨çš„ FFT é•·åº¦ï¼Œxreal èˆ‡ ximag ç‚º FFT è½‰æ›å¾Œçš„çµæœ\n",
    "    \"\"\"\n",
    "    # æ‰¾åˆ°ä¸è¶…é len(xreal) çš„æœ€å¤§ 2 çš„å†ªæ¬¡\n",
    "    n = 2\n",
    "    while n * 2 <= len(xreal):\n",
    "        n *= 2\n",
    "    p = int(math.log(n, 2))\n",
    "    \n",
    "    # Bit reversal æ’åº\n",
    "    for i in range(n):\n",
    "        a = i\n",
    "        b = 0\n",
    "        for j in range(p):\n",
    "            b = b * 2 + (a % 2)\n",
    "            a = a // 2\n",
    "        if b > i:\n",
    "            xreal[i], xreal[b] = xreal[b], xreal[i]\n",
    "            ximag[i], ximag[b] = ximag[b], ximag[i]\n",
    "    \n",
    "    # æº–å‚™ twiddle factors\n",
    "    wreal = [1.0]\n",
    "    wimag = [0.0]\n",
    "    arg = -2 * math.pi / n\n",
    "    treal = math.cos(arg)\n",
    "    timag = math.sin(arg)\n",
    "    for j in range(1, n // 2):\n",
    "        wreal.append(wreal[-1] * treal - wimag[-1] * timag)\n",
    "        wimag.append(wreal[-1] * timag + wimag[-1] * treal)\n",
    "    \n",
    "    m = 2\n",
    "    while m <= n:\n",
    "        for k in range(0, n, m):\n",
    "            for j in range(m // 2):\n",
    "                index1 = k + j\n",
    "                index2 = index1 + m // 2\n",
    "                t = int(n * j / m)\n",
    "                treal_temp = wreal[t] * xreal[index2] - wimag[t] * ximag[index2]\n",
    "                timag_temp = wreal[t] * ximag[index2] + wimag[t] * xreal[index2]\n",
    "                ureal = xreal[index1]\n",
    "                uimag = ximag[index1]\n",
    "                xreal[index1] = ureal + treal_temp\n",
    "                ximag[index1] = uimag + timag_temp\n",
    "                xreal[index2] = ureal - treal_temp\n",
    "                ximag[index2] = uimag - timag_temp\n",
    "        m *= 2\n",
    "\n",
    "    return n, xreal, ximag\n",
    "\n",
    "def FFT_data(input_data, swinging_times):\n",
    "    \"\"\"\n",
    "    ä¾æ“šå„æ“ºå‹•å€æ®µæ™‚é–“é»è¨ˆç®—å€æ®µå…§çš„åŠ é€Ÿåº¦èˆ‡é™€èºå„€å‘é‡å‡å€¼\n",
    "    åƒæ•¸:\n",
    "      input_data    : 2D æ•¸æ“šï¼Œæ¯åˆ—ç‚º [Ax, Ay, Az, Gx, Gy, Gz]\n",
    "      swinging_times: æ“ºå‹•å€æ®µèµ·å§‹ index åˆ—è¡¨ï¼Œä¾‹å¦‚ [start0, start1, ..., end]\n",
    "    å›å‚³:\n",
    "      a_mean, g_mean: æ¯ä¸€å€æ®µçš„åŠ é€Ÿåº¦èˆ‡é™€èºå„€å‘é‡å‡å€¼ï¼ˆä¾åºæ’åˆ—ï¼‰\n",
    "    \"\"\"\n",
    "    num_segments = len(swinging_times) - 1\n",
    "    a_mean = [0] * num_segments\n",
    "    g_mean = [0] * num_segments\n",
    "    \n",
    "    for i in range(num_segments):\n",
    "        a_vals = []\n",
    "        g_vals = []\n",
    "        for j in range(swinging_times[i], swinging_times[i+1]):\n",
    "            a_val = math.sqrt(input_data[j][0]**2 + input_data[j][1]**2 + input_data[j][2]**2)\n",
    "            g_val = math.sqrt(input_data[j][3]**2 + input_data[j][4]**2 + input_data[j][5]**2)\n",
    "            a_vals.append(a_val)\n",
    "            g_vals.append(g_val)\n",
    "        a_mean[i] = sum(a_vals) / len(a_vals) if a_vals else 0\n",
    "        g_mean[i] = sum(g_vals) / len(g_vals) if g_vals else 0\n",
    "    return a_mean, g_mean\n",
    "\n",
    "def extract_fft_features(signal):\n",
    "    \"\"\"\n",
    "    ä»¥æ‰‹å‹• FFT è¨ˆç®—ä¿¡è™Ÿçš„é »åŸŸç‰¹å¾µ\n",
    "    åƒæ•¸:\n",
    "      signal: ä¸€ç¶­æ•¸æ“šï¼Œä»£è¡¨å–®ä¸€è»¸çš„æ•¸å€¼åºåˆ—\n",
    "    å›å‚³:\n",
    "      å­—å…¸å½¢å¼çš„é »åŸŸç‰¹å¾µï¼ŒåŒ…æ‹¬å…‰è­œç†µã€å³°åº¦ã€ååº¦èˆ‡å‡å€¼\n",
    "    \"\"\"\n",
    "    L = len(signal)\n",
    "    # è£œé›¶è‡³ 2 çš„å†ªæ¬¡\n",
    "    n_fft = 1\n",
    "    while n_fft < L:\n",
    "        n_fft *= 2\n",
    "    sig_list = list(signal)\n",
    "    sig_list += [0] * (n_fft - L)\n",
    "    xreal = sig_list.copy()\n",
    "    ximag = [0] * n_fft\n",
    "    n, fft_real, fft_imag = FFT(xreal, ximag)\n",
    "    \n",
    "    # è¨ˆç®—åŠŸç‡è­œå¯†åº¦ï¼ˆPSDï¼‰\n",
    "    psd = [fft_real[i]**2 + fft_imag[i]**2 for i in range(n)]\n",
    "    total_power = sum(psd)\n",
    "    if total_power > 0:\n",
    "        psd_norm = [p / total_power for p in psd]\n",
    "    else:\n",
    "        psd_norm = [0] * len(psd)\n",
    "    fft_entropy_val = entropy(psd_norm) if total_power > 0 else 0\n",
    "    fft_kurtosis_val = kurtosis(psd)\n",
    "    fft_skewness_val = skew(psd)\n",
    "    fft_power_val = np.mean(psd)\n",
    "    \n",
    "    return {\n",
    "        'fft_entropy': fft_entropy_val,\n",
    "        'fft_kurtosis': fft_kurtosis_val,\n",
    "        'fft_skewness': fft_skewness_val,\n",
    "        'fft_power': fft_power_val\n",
    "    }\n",
    "\n",
    "def extract_segment_features(seg):\n",
    "    \"\"\"\n",
    "    é‡å°å–®ä¸€æ®µ (seg) è¨ˆç®—æ‰€æœ‰è»¸çš„æ™‚é–“åŸŸçµ±è¨ˆç‰¹å¾µèˆ‡ä½¿ç”¨æ‰‹å‹• FFT å–å¾—çš„é »åŸŸç‰¹å¾µ\n",
    "    \"\"\"\n",
    "    features = {}\n",
    "    axis_names = [\"Ax\", \"Ay\", \"Az\", \"Gx\", \"Gy\", \"Gz\"]\n",
    "    for i, name in enumerate(axis_names):\n",
    "        axis_data = seg[:, i]\n",
    "        features[f\"{name}_mean\"] = np.mean(axis_data)\n",
    "        features[f\"{name}_std\"] = np.std(axis_data)\n",
    "        features[f\"{name}_rms\"] = np.sqrt(np.mean(axis_data**2))\n",
    "        features[f\"{name}_min\"] = np.min(axis_data)\n",
    "        features[f\"{name}_max\"] = np.max(axis_data)\n",
    "        fft_feat = extract_fft_features(axis_data)\n",
    "        for k, v in fft_feat.items():\n",
    "            features[f\"{name}_{k}\"] = v\n",
    "    # é¡å¤–è¨ˆç®—ä¸‰è»¸åŠ é€Ÿåº¦èˆ‡é™€èºå„€å‘é‡\n",
    "    acc = np.linalg.norm(seg[:, :3], axis=1)\n",
    "    gyro = np.linalg.norm(seg[:, 3:], axis=1)\n",
    "    features[\"acc_mean\"] = np.mean(acc)\n",
    "    features[\"gyro_mean\"] = np.mean(gyro)\n",
    "    features[\"segment_length\"] = len(seg)\n",
    "    return features\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# åŸå§‹è³‡æ–™è™•ç†èˆ‡ç‰¹å¾µæ“·å–æµç¨‹ï¼ˆç›´æ¥ç”¨ FFT ç‰¹å¾µï¼‰\n",
    "# =============================================================================\n",
    "def analyze_segment_statistics(info_df, data_dir):\n",
    "    lengths, rms_list, p2p_list = [], [], []\n",
    "    for _, row in info_df.iterrows():\n",
    "        uid = row[\"unique_id\"]\n",
    "        txt_path = os.path.join(data_dir, f\"{uid}.txt\")\n",
    "        try:\n",
    "            raw = np.loadtxt(txt_path)\n",
    "            cut = [int(x) for x in row[\"cut_point\"].strip(\"[]\").split()]\n",
    "            cut = [0] + cut + [len(raw)]\n",
    "            segments = [raw[cut[i]:cut[i+1]] for i in range(min(27, len(cut)-1))]\n",
    "            for seg in segments:\n",
    "                if len(seg) < 3:\n",
    "                    continue\n",
    "                acc = np.linalg.norm(seg[:, :3], axis=1)\n",
    "                rms = np.sqrt(np.mean(acc**2))\n",
    "                p2p = np.ptp(acc)\n",
    "                lengths.append(len(seg))\n",
    "                rms_list.append(rms)\n",
    "                p2p_list.append(p2p)\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    def plot_and_summary(data, name):\n",
    "        plt.figure(figsize=(8, 4))\n",
    "        sns.histplot(data, bins=40, kde=True)\n",
    "        plt.title(f\"{name} Distribution\")\n",
    "        plt.xlabel(name)\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        mean_val = np.mean(data)\n",
    "        std_val = np.std(data)\n",
    "        q25, q75 = np.percentile(data, [25, 75])\n",
    "        print(f\"\\nğŸ“Š {name} Summary:\")\n",
    "        print(f\"  Mean: {mean_val:.3f}, Std: {std_val:.3f}, Q1: {q25:.2f}, Q3: {q75:.2f}, IQR: {q75 - q25:.2f}\")\n",
    "        lower = q25 - 1.5 * (q75 - q25)\n",
    "        upper = q75 + 1.5 * (q75 - q25)\n",
    "        print(f\"  å»ºè­°ç¯©é¸å€é–“: [{lower:.1f}, {upper:.1f}]\\n\")\n",
    "\n",
    "    plot_and_summary(lengths, \"Segment Length\")\n",
    "    plot_and_summary(rms_list, \"RMS Acceleration\")\n",
    "    plot_and_summary(p2p_list, \"Peak-to-Peak Acceleration\")\n",
    "\n",
    "def is_valid_segment(seg):\n",
    "    length = len(seg)\n",
    "    if length < 57 or length > 129:\n",
    "        return False\n",
    "    acc = np.linalg.norm(seg[:, :3], axis=1)\n",
    "    rms = np.sqrt(np.mean(acc**2))\n",
    "    p2p = np.ptp(acc)\n",
    "    if rms < 2500 or p2p < 3000:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def create_feature_dataframe(info_df, data_dir, with_labels=True):\n",
    "    rows = []\n",
    "    for _, row in info_df.iterrows():\n",
    "        uid = row[\"unique_id\"]\n",
    "        txt_path = os.path.join(data_dir, f\"{uid}.txt\")\n",
    "        try:\n",
    "            raw_data = np.loadtxt(txt_path)\n",
    "            cut_points = [int(x) for x in row[\"cut_point\"].strip(\"[]\").split()]\n",
    "            cut_points = [0] + cut_points + [len(raw_data)]\n",
    "            segments = [raw_data[cut_points[i]:cut_points[i+1]] for i in range(min(27, len(cut_points)-1))]\n",
    "            for seg in segments:\n",
    "                if not is_valid_segment(seg):\n",
    "                    continue\n",
    "                feat = extract_segment_features(seg)\n",
    "                feat[\"unique_id\"] = uid\n",
    "                if with_labels:\n",
    "                    feat[\"gender\"] = 1 if row[\"gender\"] == 1 else 0\n",
    "                    feat[\"hand\"] = 1 if row[\"hold racket handed\"] == 1 else 0\n",
    "                    feat[\"play\"] = row[\"play years\"]\n",
    "                    feat[\"level\"] = row[\"level\"]\n",
    "                rows.append(feat)\n",
    "        except Exception as e:\n",
    "            continue\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# =============================================================================\n",
    "# æ¨¡å‹è¨“ç·´èˆ‡é©—è­‰\n",
    "# =============================================================================\n",
    "# å»ºç«‹è¨“ç·´è³‡æ–™èˆ‡é©—è­‰é›†\n",
    "train_df = create_feature_dataframe(train_info, os.path.join(train_inner, \"train_data\"), with_labels=True)\n",
    "train_ids, val_ids = train_test_split(train_df[\"unique_id\"].unique(), test_size=0.2, random_state=42)\n",
    "train_part = train_df[train_df[\"unique_id\"].isin(train_ids)]\n",
    "val_part   = train_df[train_df[\"unique_id\"].isin(val_ids)]\n",
    "\n",
    "# æ¨™æº–åŒ–ï¼ˆç§»é™¤æ¨™ç±¤æ¬„ä½ï¼‰\n",
    "X_train = train_part.drop(columns=[\"unique_id\", \"gender\", \"hand\", \"play\", \"level\"])\n",
    "X_val   = val_part.drop(columns=[\"unique_id\", \"gender\", \"hand\", \"play\", \"level\"])\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_val_scaled   = scaler.transform(X_val)\n",
    "\n",
    "# æ¨¡å‹è¨“ç·´ï¼ˆä»¥ LogisticRegression ç‚ºä¾‹ï¼‰\n",
    "gender_model = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "hand_model   = LogisticRegression(solver=\"lbfgs\", max_iter=1000)\n",
    "play_model   = LogisticRegression(multi_class=\"multinomial\", solver=\"saga\", max_iter=1000)\n",
    "level_model  = LogisticRegression(multi_class=\"multinomial\", solver=\"saga\", max_iter=1000)\n",
    "\n",
    "gender_model.fit(X_train_scaled, train_part[\"gender\"])\n",
    "hand_model.fit(X_train_scaled, train_part[\"hand\"])\n",
    "play_model.fit(X_train_scaled, train_part[\"play\"])\n",
    "level_model.fit(X_train_scaled, train_part[\"level\"])\n",
    "\n",
    "def evaluate_on_val_set(val_df, gender_model, hand_model, play_model, level_model, scaler):\n",
    "    print(\"\\nğŸ” è©•ä¼°é©—è­‰é›†...\")\n",
    "    grouped = val_df.groupby(\"unique_id\")\n",
    "    preds = []\n",
    "    truths = []\n",
    "    for uid, group in grouped:\n",
    "        X = group.drop(columns=[\"unique_id\", \"gender\", \"hand\", \"play\", \"level\"])\n",
    "        y = group.iloc[0]  # åŒä¸€ uid æ¨™ç±¤ç›¸åŒï¼Œåªå–ç¬¬ä¸€ç­†å³å¯\n",
    "        X_scaled = scaler.transform(X)\n",
    "        gender_p = gender_model.predict_proba(X_scaled)[:, 1].mean()\n",
    "        hand_p   = hand_model.predict_proba(X_scaled)[:, 1].mean()\n",
    "        play_p   = play_model.predict_proba(X_scaled).mean(axis=0)\n",
    "        level_p  = level_model.predict_proba(X_scaled).mean(axis=0)\n",
    "        preds.append({\n",
    "            \"unique_id\": uid,\n",
    "            \"gender\": gender_p,\n",
    "            \"hand\": hand_p,\n",
    "            \"play_0\": play_p[0], \"play_1\": play_p[1], \"play_2\": play_p[2],\n",
    "            \"level_2\": level_p[0], \"level_3\": level_p[1],\n",
    "            \"level_4\": level_p[2], \"level_5\": level_p[3],\n",
    "        })\n",
    "        truths.append({\n",
    "            \"gender\": y[\"gender\"],\n",
    "            \"hand\": y[\"hand\"],\n",
    "            \"play\": y[\"play\"],\n",
    "            \"level\": y[\"level\"]\n",
    "        })\n",
    "    pred_df = pd.DataFrame(preds)\n",
    "    truth_df = pd.DataFrame(truths)\n",
    "    auc_gender = roc_auc_score(truth_df[\"gender\"], pred_df[\"gender\"])\n",
    "    auc_hand   = roc_auc_score(truth_df[\"hand\"], pred_df[\"hand\"])\n",
    "    y_play = pd.get_dummies(truth_df[\"play\"])\n",
    "    y_level = pd.get_dummies(truth_df[\"level\"])\n",
    "    auc_play  = roc_auc_score(y_play, pred_df[[\"play_0\", \"play_1\", \"play_2\"]], multi_class='ovr', average=\"micro\")\n",
    "    auc_level = roc_auc_score(y_level, pred_df[[\"level_2\", \"level_3\", \"level_4\", \"level_5\"]], multi_class='ovr', average=\"micro\")\n",
    "    final_score = 0.25 * (auc_gender + auc_hand + auc_play + auc_level)\n",
    "    print(\"\\nğŸ¯ é©—è­‰é›† AUC:\")\n",
    "    print(f\"Gender : {auc_gender:.4f}\")\n",
    "    print(f\"Hand   : {auc_hand:.4f}\")\n",
    "    print(f\"Play   : {auc_play:.4f}\")\n",
    "    print(f\"Level  : {auc_level:.4f}\")\n",
    "    print(f\"Final Score: {final_score:.4f}\\n\")\n",
    "    return final_score\n",
    "\n",
    "evaluate_on_val_set(val_part, gender_model, hand_model, play_model, level_model, scaler)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# Test set é æ¸¬ä¸¦è¼¸å‡ºçµæœ\n",
    "# =============================================================================\n",
    "print(\"é–‹å§‹ Test set é æ¸¬...\")\n",
    "all_results = []\n",
    "for uid in test_info[\"unique_id\"]:\n",
    "    txt_path = os.path.join(test_inner, \"test_data\", f\"{uid}.txt\")\n",
    "    try:\n",
    "        raw_data = np.loadtxt(txt_path)\n",
    "        row = test_info[test_info[\"unique_id\"] == uid].iloc[0]\n",
    "        cut_points = [int(x) for x in row[\"cut_point\"].strip(\"[]\").split()]\n",
    "        cut_points = [0] + cut_points + [len(raw_data)]\n",
    "        segments = [raw_data[cut_points[i]:cut_points[i+1]] for i in range(min(27, len(cut_points)-1))]\n",
    "        valid_segments = [extract_segment_features(seg) for seg in segments if is_valid_segment(seg)]\n",
    "        # è‹¥ç„¡ segment ç¬¦åˆæ¢ä»¶ï¼Œå–å‰ 100 ç­†åš fallback\n",
    "        if len(valid_segments) == 0:\n",
    "            fallback_seg = raw_data[:100] if len(raw_data) >= 100 else raw_data\n",
    "            valid_segments = [extract_segment_features(fallback_seg)]\n",
    "        df_feat = pd.DataFrame(valid_segments)\n",
    "        X_scaled = scaler.transform(df_feat)\n",
    "        gender_prob = gender_model.predict_proba(X_scaled)[:, 1].mean()\n",
    "        hand_prob   = hand_model.predict_proba(X_scaled)[:, 1].mean()\n",
    "        play_prob   = play_model.predict_proba(X_scaled).mean(axis=0)\n",
    "        level_prob  = level_model.predict_proba(X_scaled).mean(axis=0)\n",
    "        all_results.append({\n",
    "            \"unique_id\": uid,\n",
    "            \"gender\": round(gender_prob, 4),\n",
    "            \"hold racket handed\": round(hand_prob, 4),\n",
    "            \"play years_0\": round(play_prob[0], 4),\n",
    "            \"play years_1\": round(play_prob[1], 4),\n",
    "            \"play years_2\": round(play_prob[2], 4),\n",
    "            \"level_2\": round(level_prob[0], 4),\n",
    "            \"level_3\": round(level_prob[1], 4),\n",
    "            \"level_4\": round(level_prob[2], 4),\n",
    "            \"level_5\": round(level_prob[3], 4)\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ç„¡æ³•è™•ç† {uid}: {e}\")\n",
    "        continue\n",
    "\n",
    "sub = pd.DataFrame(all_results)\n",
    "sub = sub[[\"unique_id\", \"gender\", \"hold racket handed\",\n",
    "           \"play years_0\", \"play years_1\", \"play years_2\",\n",
    "           \"level_2\", \"level_3\", \"level_4\", \"level_5\"]]\n",
    "\n",
    "sample_path = r\"C:\\Users\\User\\Downloads\\sample_submission.csv\"\n",
    "if os.path.exists(sample_path):\n",
    "    sample = pd.read_csv(sample_path)\n",
    "    missing_ids = set(sample[\"unique_id\"]) - set(sub[\"unique_id\"])\n",
    "    extra_ids = set(sub[\"unique_id\"]) - set(sample[\"unique_id\"])\n",
    "    print(f\"æª¢æŸ¥ ID æ•¸é‡ï¼šé æœŸ {len(sample)}ï¼Œç›®å‰ç”¢å‡º {len(sub)}\")\n",
    "    if missing_ids:\n",
    "        print(f\"âŒ ç¼ºå°‘ ID: {sorted(list(missing_ids))[:5]} ... å…± {len(missing_ids)} ç­†\")\n",
    "    if extra_ids:\n",
    "        print(f\"âš ï¸ å¤šå‡º ID: {sorted(list(extra_ids))[:5]} ... å…± {len(extra_ids)} ç­†\")\n",
    "    if not missing_ids and not extra_ids:\n",
    "        print(\"âœ… æ‰€æœ‰ ID èˆ‡ sample_submission.csv ä¸€è‡´ï¼\")\n",
    "else:\n",
    "    print(f\"âš ï¸ æ‰¾ä¸åˆ° sample_submission.csvï¼š{sample_path}\")\n",
    "\n",
    "sub.to_csv(\"sample_submission_predtry.csv\", index=False)\n",
    "print(\"âœ… çµæœå·²è¼¸å‡ºè‡³ sample_submission_predtry.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
